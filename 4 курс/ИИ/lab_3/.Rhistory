View(swiss)
swiss = read.table("./swiss.txt", header = TRUE, row.names = 1)
swiss = read.table("./swiss.txt", header = TRUE, row.names = 1)
swiss = read.table("./swiss.txt", header = TRUE, row.names = 1)
swiss = read.table("./swiss.txt")
swiss = read.table("./votes.txt")
swiss = read.table("./votes.repub.txt")
swiss_pca = prcomp(swiss)
swiss = read.table("./votes.repub.txt", header = TRUE)
swiss_pca = prcomp(swiss)
swiss = read.table("./votes.repub.txt", header = TRUE, row.names = 1)
swiss_pca = prcomp(swiss)
View(swiss)
swiss = read.table("./swiss.txt", header = TRUE, row.names = 1)
swiss = read.table("./swissdata.txt", header = TRUE, row.names = 1)
swiss = read.table("./swiss.txt", header = TRUE, row.names = 1)
swiss = read.table("./swiss.txt", header = TRUE, row.names = 1)
swiss = read.table("./swiss.txt", header = TRUE, row.names = 1)
swiss = read.table("./swiss.txt", header = TRUE, row.names = 1)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
View(swiss)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = colnames(swiss),
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки", barplot.default(names.arg = colnames(swiss)),
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки", barplot.default(height = 80, names.arg = colnames(swiss)),
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки", labels(colnames(swiss)),
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки", labels = colnames(swiss),
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
eig_plot + scale_x_discrete(labels = colnames(swiss))
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
eig_plot + scale_x_discrete(labels = colnames(swiss))
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости" +
scale_x_discrete(labels = colnames(swiss))  )
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
graph = fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
graph  +
scale_x_discrete(labels = colnames(swiss))
graph
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(scale(swiss))
graph = fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
graph  +
scale_x_discrete(labels = colnames(swiss))
graph
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
graph = fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
graph  +
scale_x_discrete(labels = colnames(swiss))
graph
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
# Чтение данных
library("dplyr")
install.packages("dplyr")
# Чтение данных
library("dplyr")
votes = read.table("./votes.repub.txt", header = TRUE)
# Избавляемся от NA
votes[is.na(votes)] = 0
# Заменим номера строк названиями шататов
rownames(votes) = votes$States
# Делаем срез, т.к. для дендрограммы важны голоса, а не штаты
votes = votes[,2:32]
# Получаем расстояния между точками
votes_d = dist(scale(votes))^2
# Получаем кластеризацию
votes_h = hclust(votes_d, method = "ward.D")
plot(votes_h, main = "Дендрограма votes.repub")
# Выделим на графике 3 кластера
rect.hclust(votes_h, k = 3, border="red")
groups = cutree(votes_h, k = 3)
groups
#votes = votes %>% mutate(groups = factor(groups))
#ggplot(data = votes, aes(x = votes$X1972 , y = votes$X1956, color = groups)) + geom_point()
library(factoextra)
install.packages("factoextra")
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
# Загрузка набора данных swiss
data(swiss)
# Стандартизация данных
swiss_scaled <- scale(swiss)
# Вычисление главных компонент
pca_model <- prcomp(swiss_scaled, center = TRUE, scale. = TRUE)
# Процент объяснённой дисперсии
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
# Минимальное число компонент, объясняющих >90% изменчивости
min_components <- which(explained_variance > 0.9)[1]
cat("Минимальное число компонент:", min_components, "\n")
# Добавление значений компонент в исходный набор данных
principal_components <- as.data.frame(pca_model$x[, 1:min_components])
colnames(principal_components) <- paste0("PC", 1:min_components)
swiss_with_pcs <- cbind(swiss, principal_components)
# Вывод результата
print("Изменчивость, объясняемая компонентами:")
print(explained_variance)
print(head(swiss_with_pcs))
# Загрузка набора данных pluton
data(pluton)
# Функция для оценки качества кластеризации при разном числе итераций
set.seed(123)
max_iter_values <- c(10, 50, 100, 200)
results <- list()
clusters_list <- list()
for (max_iter in max_iter_values) {
kmeans_model <- kmeans(pluton, centers = 3, iter.max = max_iter, nstart = 10)
results[[as.character(max_iter)]] <- kmeans_model$tot.withinss
clusters_list[[as.character(max_iter)]] <- kmeans_model$cluster
}
# График зависимости качества кластеризации от числа итераций
plot(max_iter_values, unlist(results), type = "b", pch = 19, col = "blue",
xlab = "Максимальное число итераций", ylab = "Сумма внутрикластерных отклонений",
main = "Качество кластеризации (K-means)")
# Визуализация кластеров для последней итерации
library(cluster)
clusplot(pluton, clusters_list[[as.character(max(max_iter_values))]],
color = TRUE, shade = TRUE, labels = 2, lines = 0,
main = "Распределение объектов по кластерам (K-means)")
# Загрузка набора данных pluton
data(pluton)
# Функция для оценки качества кластеризации при разном числе итераций
set.seed(123)
max_iter_values <- c(10, 50, 100, 200)
results <- list()
clusters_list <- list()
for (max_iter in max_iter_values) {
kmeans_model <- kmeans(pluton, centers = 3, iter.max = max_iter, nstart = 10)
results[[as.character(max_iter)]] <- kmeans_model$tot.withinss
clusters_list[[as.character(max_iter)]] <- kmeans_model$cluster
}
# График зависимости качества кластеризации от числа итераций
plot(max_iter_values, unlist(results), type = "b", pch = 19, col = "blue",
xlab = "Максимальное число итераций", ylab = "Сумма внутрикластерных отклонений",
main = "Качество кластеризации (K-means)")
# Визуализация кластеров для последней итерации
library(cluster)
clusplot(pluton, clusters_list[[as.character(max(max_iter_values))]],
color = TRUE, shade = TRUE, labels = 2, lines = 0,
main = "Распределение объектов по кластерам (K-means)")
# Подключение библиотеки визуализации
library(factoextra)
# Чтение данных
pluton = read.table("./pluton.txt", header = TRUE)
# Кол-во кластеров
k = 3
# Кол-во итераций
# При iters < 6 – нечеткое разделение
# При iters >= 6 – ровное разделение на 3 кластера
iters = 6
# Кластеризация методом k-средних
clusters = kmeans(pluton, k, nstart = iters)
# Отображение результата кластеризации на исходном датасете
fviz_cluster(clusters, pluton, main = "Кластеризация плутона",
xlab = FALSE, ylab = FALSE)
# Загрузка набора данных pluton
data(pluton)
# Функция для оценки качества кластеризации при разном числе итераций
set.seed(123)
max_iter_values <- c(10, 50, 100, 200)
results <- list()
clusters_list <- list()
for (max_iter in max_iter_values) {
kmeans_model <- kmeans(pluton, centers = 3, iter.max = max_iter, nstart = 10)
results[[as.character(max_iter)]] <- kmeans_model$tot.withinss
clusters_list[[as.character(max_iter)]] <- kmeans_model$cluster
}
# График зависимости качества кластеризации от числа итераций
plot(max_iter_values, unlist(results), type = "b", pch = 19, col = "blue",
xlab = "Максимальное число итераций", ylab = "Сумма внутрикластерных отклонений",
main = "Качество кластеризации (K-means)")
# Визуализация кластеров для последней итерации
library(cluster)
clusplot(pluton, clusters_list[[as.character(max(max_iter_values))]],
color = TRUE, shade = TRUE, labels = 2, lines = 0,
main = "Распределение объектов по кластерам (K-means)")
# Подключение библиотеки визуализации
library(factoextra)
# Чтение данных
pluton = read.table("./pluton.txt", header = TRUE)
# Кол-во кластеров
k = 3
# Кол-во итераций
# При iters < 6 – нечеткое разделение
# При iters >= 6 – ровное разделение на 3 кластера
iters = 6
# Кластеризация методом k-средних
clusters = kmeans(pluton, k, nstart = iters)
# Отображение результата кластеризации на исходном датасете
fviz_cluster(clusters, pluton, main = "Кластеризация плутона",
xlab = FALSE, ylab = FALSE)
# Загрузка набора данных votes.repub
data(votes.repub)
# Вычисление матрицы расстояний и иерархической кластеризации
dist_matrix <- dist(votes.repub)
hclust_model <- hclust(dist_matrix, method = "ward.D2")
# Построение дендрограммы
plot(hclust_model, main = "Дендрограмма для данных votes.repub",
xlab = "Штаты", ylab = "Расстояние", sub = "", cex = 0.7)
# Раскраска дендрограммы (опционально, если нужно разделить на кластеры)
rect.hclust(hclust_model, k = 4, border = "red")
# Загрузка набора данных votes.repub
data(votes.repub)
# Вычисление матрицы расстояний и иерархической кластеризации
dist_matrix <- dist(votes.repub)
hclust_model <- hclust(dist_matrix, method = "ward.D2")
# Построение дендрограммы
plot(hclust_model, main = "Дендрограмма для данных votes.repub",
xlab = "Штаты", ylab = "Расстояние", sub = "", cex = 0.7)
# Раскраска дендрограммы (опционально, если нужно разделить на кластеры)
rect.hclust(hclust_model, k = 3, border = "red")
# Чтение данных
library("dplyr")
votes = read.table("./votes.repub.txt", header = TRUE)
# Избавляемся от NA
votes[is.na(votes)] = 0
# Заменим номера строк названиями шататов
rownames(votes) = votes$States
# Делаем срез, т.к. для дендрограммы важны голоса, а не штаты
votes = votes[,2:32]
# Получаем расстояния между точками
votes_d = dist(scale(votes))^2
# Получаем кластеризацию
votes_h = hclust(votes_d, method = "ward.D")
plot(votes_h, main = "Дендрограма votes.repub")
# Выделим на графике 3 кластера
rect.hclust(votes_h, k = 3, border="red")
groups = cutree(votes_h, k = 3)
groups
#votes = votes %>% mutate(groups = factor(groups))
#ggplot(data = votes, aes(x = votes$X1972 , y = votes$X1956, color = groups)) + geom_point()
# Загрузка набора данных votes.repub
data(votes.repub)
# Вычисление матрицы расстояний и иерархической кластеризации
dist_matrix <- dist(votes.repub)
hclust_model <- hclust(dist_matrix, method = "ward.D2")
# Построение дендрограммы
plot(hclust_model, main = "Дендрограмма для данных votes.repub",
xlab = "Штаты", ylab = "Расстояние", sub = "", cex = 0.7)
# Раскраска дендрограммы (опционально, если нужно разделить на кластеры)
rect.hclust(hclust_model, k = 3, border = "red")
# Чтение данных
library("dplyr")
votes = read.table("./votes.repub.txt", header = TRUE)
# Избавляемся от NA
votes[is.na(votes)] = 0
# Заменим номера строк названиями шататов
rownames(votes) = votes$States
# Делаем срез, т.к. для дендрограммы важны голоса, а не штаты
votes = votes[,2:32]
# Получаем расстояния между точками
votes_d = dist(scale(votes))^2
# Получаем кластеризацию
votes_h = hclust(votes_d, method = "ward.D")
plot(votes_h, main = "Дендрограма votes.repub", xlab = "Штаты")
# Выделим на графике 3 кластера
rect.hclust(votes_h, k = 3, border="red")
groups = cutree(votes_h, k = 3)
groups
#votes = votes %>% mutate(groups = factor(groups))
#ggplot(data = votes, aes(x = votes$X1972 , y = votes$X1956, color = groups)) + geom_point()
# Чтение данных
library("dplyr")
votes = read.table("./votes.repub.txt", header = TRUE)
# Избавляемся от NA
votes[is.na(votes)] = 0
# Заменим номера строк названиями шататов
rownames(votes) = votes$States
# Делаем срез, т.к. для дендрограммы важны голоса, а не штаты
votes = votes[,2:32]
# Получаем расстояния между точками
votes_d = dist(scale(votes))^2
# Получаем кластеризацию
votes_h = hclust(votes_d, method = "ward.D")
plot(votes_h, main = "Дендрограма votes.repub", xlab = "Штаты")
# Выделим на графике 3 кластера
rect.hclust(votes_h, k = 3, border="red")
# Чтение данных
library("dplyr")
votes = read.table("./votes.repub.txt", header = TRUE)
# Избавляемся от NA
votes[is.na(votes)] = 0
# Заменим номера строк названиями шататов
rownames(votes) = votes$States
# Делаем срез, т.к. для дендрограммы важны голоса, а не штаты
votes = votes[,2:32]
# Получаем расстояния между точками
votes_d = dist(scale(votes))^2
# Получаем кластеризацию
votes_h = hclust(votes_d, method = "ward.D")
plot(votes_h, main = "Дендрограма votes.repub", xlab = "Штаты", ylab = "Расстояние")
# Выделим на графике 3 кластера
rect.hclust(votes_h, k = 3, border="red")
# Загрузка набора данных votes.repub
data(votes.repub)
# Вычисление матрицы расстояний и иерархической кластеризации
dist_matrix <- dist(votes.repub)
hclust_model <- hclust(dist_matrix, method = "ward.D2")
# Построение дендрограммы
plot(hclust_model, main = "Дендрограмма для данных votes.repub",
xlab = "Штаты", ylab = "Расстояние", sub = "", cex = 0.7)
# Раскраска дендрограммы (опционально, если нужно разделить на кластеры)
rect.hclust(hclust_model, k = 3, border = "red")
# Загрузка набора данных swiss
data(swiss)
# Стандартизация данных
swiss_scaled <- scale(swiss)
# Вычисление главных компонент
pca_model <- prcomp(swiss_scaled, center = TRUE, scale. = TRUE)
# Процент объяснённой дисперсии
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
# Минимальное число компонент, объясняющих >90% изменчивости
min_components <- which(explained_variance > 0.9)[1]
cat("Минимальное число компонент:", min_components, "\n")
# Построение графика объяснённой дисперсии
plot(explained_variance, type = "b", pch = 19, col = "blue",
xlab = "Число компонент", ylab = "Объяснённая дисперсия",
main = "Кумулятивная объяснённая дисперсия")
abline(h = 0.9, col = "red", lty = 2)
# Добавление значений компонент в исходный набор данных
principal_components <- as.data.frame(pca_model$x[, 1:min_components])
colnames(principal_components) <- paste0("PC", 1:min_components)
swiss_with_pcs <- cbind(swiss, principal_components)
# График двух первых главных компонент
plot(pca_model$x[, 1:2], col = "blue", pch = 19,
xlab = "Главная компонента 1", ylab = "Главная компонента 2",
main = "Проекция на первые две главные компоненты")
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
# Чтение данных
library("dplyr")
votes = read.table("./votes.repub.txt", header = TRUE)
# Избавляемся от NA
votes[is.na(votes)] = 0
# Заменим номера строк названиями шататов
rownames(votes) = votes$States
# Делаем срез, т.к. для дендрограммы важны голоса, а не штаты
votes = votes[,2:32]
# Получаем расстояния между точками
votes_d = dist(scale(votes))^2
# Получаем кластеризацию
votes_h = hclust(votes_d, method = "ward.D")
plot(votes_h, main = "Дендрограма votes.repub", xlab = "Штаты", ylab = "Расстояние")
# Выделим на графике 3 кластера
rect.hclust(votes_h, k = 3, border="red")
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент", ylab = "Процент объяснения изменчивости")
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент",
ylab = "Процент объяснения изменчивости")
View(swiss)
library(factoextra)
swiss = read.table("./swiss.txt", header = TRUE, sep = ",", row.names = 1)
swiss_pca = prcomp(swiss, scale = TRUE)
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент",
ylab = "Процент объяснения изменчивости")
# Загрузка набора данных swiss
data(swiss)
# Стандартизация данных
swiss_scaled <- scale(swiss)
# Вычисление главных компонент
pca_model <- prcomp(swiss_scaled, center = TRUE, scale. = TRUE)
# Процент объяснённой дисперсии
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
# Минимальное число компонент, объясняющих >90% изменчивости
min_components <- which(explained_variance > 0.9)[1]
cat("Минимальное число компонент:", min_components, "\n")
# Построение графика объяснённой дисперсии
plot(explained_variance, type = "b", pch = 19, col = "blue",
xlab = "Число компонент", ylab = "Объяснённая дисперсия",
main = "Кумулятивная объяснённая дисперсия")
abline(h = 0.9, col = "red", lty = 2)
# Добавление значений компонент в исходный набор данных
principal_components <- as.data.frame(pca_model$x[, 1:min_components])
colnames(principal_components) <- paste0("PC", 1:min_components)
swiss_with_pcs <- cbind(swiss, principal_components)
# График двух первых главных компонент
plot(pca_model$x[, 1:2], col = "blue", pch = 19,
xlab = "Главная компонента 1", ylab = "Главная компонента 2",
main = "Проекция на первые две главные компоненты")
# Загрузка набора данных swiss
data(swiss)
# Стандартизация данных
swiss_scaled <- scale(swiss)
# Вычисление главных компонент
pca_model <- prcomp(swiss_scaled, center = TRUE, scale. = TRUE)
# Процент объяснённой дисперсии
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
# Минимальное число компонент, объясняющих >90% изменчивости
min_components <- which(explained_variance > 0.9)[1]
cat("Минимальное число компонент:", min_components, "\n")
# Построение графика объяснённой дисперсии
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент",
ylab = "Процент объяснения изменчивости")
# Добавление значений компонент в исходный набор данных
principal_components <- as.data.frame(pca_model$x[, 1:min_components])
colnames(principal_components) <- paste0("PC", 1:min_components)
swiss_with_pcs <- cbind(swiss, principal_components)
# График двух первых главных компонент
plot(pca_model$x[, 1:2], col = "blue", pch = 19,
xlab = "Главная компонента 1", ylab = "Главная компонента 2",
main = "Проекция на первые две главные компоненты")
# Загрузка набора данных swiss
data(swiss)
# Стандартизация данных
swiss_scaled <- scale(swiss)
# Вычисление главных компонент
pca_model <- prcomp(swiss_scaled, center = TRUE, scale. = TRUE)
# Процент объяснённой дисперсии
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
# Минимальное число компонент, объясняющих >90% изменчивости
min_components <- which(explained_variance > 0.9)[1]
cat("Минимальное число компонент:", min_components, "\n")
# Построение графика объяснённой дисперсии
fviz_eig(swiss_pca, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент",
ylab = "Процент объяснения изменчивости")
# Добавление значений компонент в исходный набор данных
principal_components <- as.data.frame(pca_model$x[, 1:min_components])
colnames(principal_components) <- paste0("PC", 1:min_components)
swiss_with_pcs <- cbind(swiss, principal_components)
View(principal_components)
# Загрузка набора данных swiss
data(swiss)
# Стандартизация данных
swiss_scaled <- scale(swiss)
# Вычисление главных компонент
pca_model <- prcomp(swiss_scaled, scale. = TRUE)
# Процент объяснённой дисперсии
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
# Минимальное число компонент, объясняющих >90% изменчивости
min_components <- which(explained_variance > 0.9)[1]
cat("Минимальное число компонент:", min_components, "\n")
# Построение графика объяснённой дисперсии
fviz_eig(swiss_scaled, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент",
ylab = "Процент объяснения изменчивости")
# Загрузка набора данных swiss
data(swiss)
# Стандартизация данных
swiss_scaled <- scale(swiss)
# Вычисление главных компонент
pca_model <- prcomp(swiss_scaled, scale. = TRUE)
# Процент объяснённой дисперсии
explained_variance <- cumsum(pca_model$sdev^2) / sum(pca_model$sdev^2)
# Минимальное число компонент, объясняющих >90% изменчивости
min_components <- which(explained_variance > 0.9)[1]
cat("Минимальное число компонент:", min_components, "\n")
# Построение графика объяснённой дисперсии
fviz_eig(pca_model, addlabels = TRUE, xlab = "Признаки",
main = "Диаграма зависимости компонент",
ylab = "Процент объяснения изменчивости")
# Добавление значений компонент в исходный набор данных
principal_components <- as.data.frame(pca_model$x[, 1:min_components])
colnames(principal_components) <- paste0("PC", 1:min_components)
swiss_with_pcs <- cbind(swiss, principal_components)
View(swiss_with_pcs)
View(swiss_with_pcs)
